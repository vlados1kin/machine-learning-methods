{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7831a69",
   "metadata": {},
   "source": [
    "# Дерево решений\n",
    "\n",
    "Дерево решений (Decision Tree) — это один из методов машинного обучения с учителем, который используется и для задач классификации (как в нашем случае с болезнями), и для регрессии.\n",
    "\n",
    "Если говорить простым языком, то это алгоритм, который принимает решение, задавая последовательность вопросов о характеристиках объекта.\n",
    "\n",
    "Профессор, если говорить конкретно о нашем датасете с болезнями сердца, то в каждом Листе (Leaf) находится финальная группа пациентов, которые прошли через все фильтры (узлы) и оказались похожими друг на друга по своим характеристикам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08f6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c25a4d",
   "metadata": {},
   "source": [
    "# 1. Загрузка и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9647e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Heart_Disease_and_Hospitals.csv')\n",
    "\n",
    "# Преобразуем категориальный признак 'gender' в числовой (0 для Female, 1 для Male)\n",
    "df['gender'] = df['gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
    "\n",
    "# Выбираем признаки (X) и целевую переменную (y)\n",
    "X = df[['age', 'blood_pressure', 'cholesterol', 'bmi', 'glucose_level', 'gender']]\n",
    "y = df['heart_disease']\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Конвертируем данные в numpy-массивы для удобства работы\n",
    "train_data = X_train.values\n",
    "train_labels = y_train.values\n",
    "test_data = X_test.values\n",
    "test_labels = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b16416",
   "metadata": {},
   "source": [
    "# 2. Реализация самописного дерева решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d946783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Алгоритм Desicion Tree с нуля\n",
    "# Узел - это \n",
    "class Node:\n",
    "    \"\"\"Класс для узла дерева (содержит условие и ветви).\"\"\"\n",
    "    def __init__(self, index, t, true_branch, false_branch):\n",
    "        self.index = index  # индекс признака для разделения\n",
    "        self.t = t  # пороговое значение\n",
    "        self.true_branch = true_branch  # ветвь для условия \"истина\"\n",
    "        self.false_branch = false_branch # ветвь для условия \"ложь\"\n",
    "\n",
    "class Leaf:\n",
    "    \"\"\"Класс для листа дерева (содержит предсказание).\"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.prediction = self.predict()\n",
    "\n",
    "    def predict(self):\n",
    "        # Cчитаем, объектов какого класса в листе больше всего\n",
    "        classes = {}\n",
    "        for label in self.labels:\n",
    "            classes[label] = classes.get(label, 0) + 1\n",
    "        # Предсказанием будет самый частый класс\n",
    "        prediction = max(classes, key=classes.get)\n",
    "        return prediction\n",
    "\n",
    "def gini(labels):\n",
    "    \"\"\"\n",
    "    (Требование 1) Расчет критерия Джини.\n",
    "    Критерий Джини - мера неоднородности выборки.\n",
    "    Мера хаоса в выборке.\n",
    "    50 больных и 50 здоровых - максимальная неоднородность (Gini = 0.5)\n",
    "    100 больных - минимальная неоднородность (Gini = 0)\n",
    "    Чем ниже значение, тем более однородна выборка.\n",
    "    Значение варьируется от 0 (все объекты одного класса) до 0.5 (объекты равномерно распределены между двумя классами).\n",
    "    0.5 - максимум для двух классов, для большего числа классов максимум меньше 0.5\n",
    "    0 - минимум\n",
    "    Рассчитывается по формуле: Gini = 1 - Σ(p_i)^2, где p_i - доля объектов класса i в выборке.\n",
    "    \"\"\"\n",
    "    # Подсчет количества объектов каждого класса\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Расчет вероятностей\n",
    "    probabilities = counts / len(labels)\n",
    "\n",
    "    # Расчет индекса Джини\n",
    "    # np.sum - сумма по всем классам\n",
    "    return 1 - np.sum(probabilities**2)\n",
    "\n",
    "def gain(left_labels, right_labels, root_gini):\n",
    "    \"\"\"\n",
    "    (Требование 2) Расчет прироста информации.\n",
    "    Это оценка полезности разделения данных по определенному признаку и порогу.\n",
    "    То есть тот, который лучше всего уменьшает неопределенность (неоднородность) в данных - уменьшает бардак.\n",
    "    Прирост информации - насколько уменьшилась неопределенность (неоднородность) после разделения.\n",
    "    Рассчитывается по формуле: Gain = Gini(родитель) - p * Gini(левый) - (1 - p) * Gini(правый),\n",
    "    где p - доля объектов, попавших в левую ветвь.\n",
    "    0 - минимум (нет прироста информации)\n",
    "    Максимум зависит от начального значения Gini(родитель) и варьируется от 0 до 0.5 для двух классов.\n",
    "    0.5 - максимум для двух классов, для большего числа классов максимум меньше 0.5\n",
    "    \"\"\"\n",
    "    # Доля левой ветви\n",
    "    p = float(len(left_labels)) / (len(left_labels) + len(right_labels))\n",
    "    # Прирост информации\n",
    "    return root_gini - p * gini(left_labels) + (1 - p) * gini(right_labels)\n",
    "\n",
    "def split(data, labels, column_index, t):\n",
    "    \"\"\"Разбиение данных на две ветви по условию.\"\"\"\n",
    "    # : - это все строки\n",
    "    # column_index - это столбец по которому мы делаем разделение\n",
    "    left_indices = np.where(data[:, column_index] <= t)\n",
    "    right_indices = np.where(data[:, column_index] > t)\n",
    "\n",
    "    return data[left_indices], data[right_indices], labels[left_indices], labels[right_indices]\n",
    "\n",
    "# Функция поиска идеального вопроса для разделения\n",
    "def find_best_split(data, labels, min_samples_leaf):\n",
    "    # Поиск лучшего признака и порога для разделения\n",
    "    root_gini = gini(labels)\n",
    "    best_gain = 0\n",
    "    best_t, best_index = None, None\n",
    "    # Количество признаков\n",
    "    n_features = data.shape[1]\n",
    "\n",
    "    for index in range(n_features):\n",
    "        # Сортируем уникальные значения признака\n",
    "        t_values = np.unique(data[:, index])\n",
    "        \n",
    "        # Используем середины между соседними значениями как пороги\n",
    "        thresholds = (t_values[:-1] + t_values[1:]) / 2\n",
    "        \n",
    "        # Перебираем все пороги\n",
    "        for t in thresholds:\n",
    "            # Разделяем данные\n",
    "            true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "\n",
    "            # Проверяем минимальное количество объектов в листе\n",
    "            if len(true_labels) < min_samples_leaf or len(false_labels) < min_samples_leaf:\n",
    "                continue\n",
    "            \n",
    "            # Вычисляем прирост информации\n",
    "            current_gain = gain(true_labels, false_labels, root_gini)\n",
    "\n",
    "            # Обновляем лучший прирост и параметры разделения\n",
    "            if current_gain > best_gain:\n",
    "                best_gain, best_t, best_index = current_gain, t, index\n",
    "\n",
    "    return best_gain, best_t, best_index\n",
    "\n",
    "def build_tree(data, labels, max_depth=5, min_samples_leaf=5, current_depth=0):\n",
    "    \"\"\"\n",
    "    (Требование 3) Рекурсивное построение дерева с критериями останова.\n",
    "    \"\"\"\n",
    "    # Критерий останова 1: Максимальная глубина\n",
    "    if current_depth >= max_depth:\n",
    "        return Leaf(data, labels)\n",
    "\n",
    "    # Поиск лучшего разделения\n",
    "    gain, t, index = find_best_split(data, labels, min_samples_leaf)\n",
    "\n",
    "    # Критерий останова 2 (неявный): Нет прироста информации\n",
    "    if gain == 0 or index is None:\n",
    "        return Leaf(data, labels)\n",
    "    \n",
    "    # Разделяем данные по лучшему признаку и порогу\n",
    "    true_data, false_data, true_labels, false_labels = split(data, labels, index, t)\n",
    "    # Рекурсивно строим ветви\n",
    "    true_branch = build_tree(true_data, true_labels, max_depth, min_samples_leaf, current_depth + 1)\n",
    "    false_branch = build_tree(false_data, false_labels, max_depth, min_samples_leaf, current_depth + 1)\n",
    "\n",
    "    # Возвращаем узел с условием\n",
    "    return Node(index, t, true_branch, false_branch)\n",
    "\n",
    "def classify_object(obj, node):\n",
    "    \"\"\"Классификация одного объекта с помощью построенного дерева.\"\"\"\n",
    "    # Если достигнут лист, возвращаем предсказание\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.prediction\n",
    "    # Иначе переходим по ветвям в зависимости от условия\n",
    "    if obj[node.index] <= node.t:\n",
    "        return classify_object(obj, node.true_branch)\n",
    "    else:\n",
    "        return classify_object(obj, node.false_branch)\n",
    "\n",
    "def predict(data, tree):\n",
    "    \"\"\"Получение предсказаний для набора данных.\"\"\"\n",
    "    # Классифицируем каждый объект в данных\n",
    "    return [classify_object(obj, tree) for obj in data]\n",
    "\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    \"\"\"\n",
    "    (Требование 4) Функция подсчета метрики (точности).\n",
    "    \"\"\"\n",
    "    # Подсчет количества правильных предсказаний\n",
    "    correct_predictions = np.sum(np.array(actual) == np.array(predicted))\n",
    "    # Вычисление точности в процентах\n",
    "    return correct_predictions / len(actual) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71663fdf",
   "metadata": {},
   "source": [
    "# 3. Обучение, оценка и сравнение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f689a5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Обучение и оценка моделей ---\n",
      "Точность самописного дерева: 50.23%\n",
      "Точность дерева из sklearn: 88.23%\n",
      "\n",
      "--- Сравнение результатов ---\n",
      "❌ Результаты не совпадают.\n",
      "Примечание: Небольшие расхождения возможны из-за различий в реализации внутренних алгоритмов (например, при обработке признаков с одинаковым приростом информации).\n"
     ]
    }
   ],
   "source": [
    "# (Требование 5)\n",
    "print(\"--- Обучение и оценка моделей ---\")\n",
    "\n",
    "# Обучение и оценка самописного дерева\n",
    "# Гиперпараметры (max_depth и min_samples_leaf) можно менять\n",
    "# max_depth=3, min_samples_leaf=5 подобраны для баланса между переобучением и недообучением\n",
    "# max_depth - максимальная глубина дерева\n",
    "# min_samples_leaf - минимальное количество объектов в листе\n",
    "my_tree = build_tree(train_data, train_labels, max_depth=3, min_samples_leaf=5)\n",
    "my_predictions = predict(test_data, my_tree)\n",
    "my_accuracy = accuracy_metric(test_labels, my_predictions)\n",
    "print(\"Точность самописного дерева: {:.2f}%\".format(my_accuracy))\n",
    "\n",
    "# Обучение и оценка дерева из sklearn\n",
    "# Используем те же гиперпараметры для корректного сравнения\n",
    "# random_state=42 для воспроизводимости результатов\n",
    "# DecisionTreeClassifier - класс из sklearn для создания дерева решений\n",
    "sk_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=5, random_state=42)\n",
    "sk_tree.fit(train_data, train_labels)\n",
    "sk_predictions = sk_tree.predict(test_data)\n",
    "sk_accuracy = accuracy_score(test_labels, sk_predictions) * 100\n",
    "print(\"Точность дерева из sklearn: {:.2f}%\".format(sk_accuracy))\n",
    "\n",
    "# Сравнение предсказаний\n",
    "print(\"\\n--- Сравнение результатов ---\")\n",
    "if np.array_equal(my_predictions, sk_predictions):\n",
    "    print(\"✅ Результаты полностью совпадают!\")\n",
    "else:\n",
    "    print(\"❌ Результаты не совпадают.\")\n",
    "    print(\"Примечание: Небольшие расхождения возможны из-за различий в реализации внутренних алгоритмов (например, при обработке признаков с одинаковым приростом информации).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
